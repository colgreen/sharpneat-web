<!DOCTYPE html>
<html lang="en">
<head>
    <title>SharpNEAT Project Roadmap</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75"/>
    <link rel="stylesheet" href="sharpneat.css" type="text/css" media="screen"/>
    <link rel="stylesheet" href="sharpneat-print.css" type="text/css" media="print"/>
</head>
<body>
    <div class="banner">      
        <a href="/" title="SharpNEAT">
            <img src="sharpneat_logo_banner.svg" alt="SharpNEAT" class="bannerimage" />
        </a>
    </div>
    <div class="centralcolumn">
        <div class="centralcolumninner">
            <!-- Column divider -->
            <div style="float: left; width: 3.3%">
                <br />
            </div>
            <!-- Pagetop Blurb -->
            <div class="textbubble">
                <div style="font-weight:bold; font-size:x-large" align="center">
                    SharpNEAT Project Roadmap
                </div>
                <p align="center">Last reviewed / updated 2023-07-26</p>
            </div>
            <br />
            <br />
            
            <div class="textbubble">
                
                <h4>Generic Math and support for Single-Precision Floats</h4>
                <p>
                    .NET 7 introduced <a href="https://devblogs.microsoft.com/dotnet/dotnet-7-generic-math/">Generic Math</a>; this allows for
                    code that performs mathematical operations on abstract numeric interface types instead of specific types (such as double and 
                    single-precision IEEE floating point numbers).
                </p>
                <p>
                    SharpNEAT currently employs double-precision floating-point arithmetic throughout, such as for connection weights and neural
                    net computations. With the introduction of Generic Math in .NET 7, it should become much simpler to support the use of
                    single-precision floats without the need to duplicate extensive amounts of code solely to change the floating point type.
                </p>
                <p>
                    Using single-precision floats should offer performance improvements. Each single-precision value needs only half the memory compared 
                    to double-precision floats. This not only improves transfer speeds between main memory and the CPU but also doubles the number of
                    values that can be accommodated in the same CPU cache space or stored in a CPU SIMD vector register.                    
                </p>

                <br/>
                <hr/>
                <h4>X86 Intrinsics</h4>
                <p>
                    SharpNEAT already contains some code that uses SIMD CPU instructions via use of the 
                    <a href="https://learn.microsoft.com/en-us/dotnet/api/system.numerics.vector-1">Vector&lt;T&gt;</a> class. However, further performance 
                    gains may be possible by using <a href="https://devblogs.microsoft.com/dotnet/hardware-intrinsics-in-net-core/">Hardware Intrinsics in
                    .NET Core</a>, as these avoid the abstraction layer provided by Vector&lt;T&gt;, and expose more of the capabilities of the underlying CPU
                    hardware. For instance, the neural net code may benefit from leveraging scatter-gather SIMD instructions, or vector dot product instructions.
                </p>

            </div>
            <br />

            <!-- Longer Term -->
            <div class="textbubble">
                <h3 align="center">Longer Term</h2>

                <h4>Integration with Native Math Libraries</h4>
                 
                <ul style="list-style-type:disc">
                    <li>
                        The ANN code can tap into highly optimized matrix-vector multiplication subroutines provided by natively compiled 
                        math libraries. In particular NEAT should benefit from sparse matrix sub-routines that fully utilise CPU and GPU 
                        capabilities such as vector/SIMD instructions, FMA (fused multiple and add) instructions, and massive parallelism.
                        Math libs of note are:
                        <ul>
                            <li><a href="https://en.wikipedia.org/wiki/Math_Kernel_Library">Intel Math Kernel Library (Intel MKL)</a></li>
                            <li><a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a></li>
                            <li><a href="https://en.wikipedia.org/wiki/LAPACK">https://en.wikipedia.org/wiki/OpenBLAS</a></li>
                        </ul>
                    </li>
                    <li>
                        Of particular note is the plug-in native math lib support in
                        <a href="https://github.com/mathnet/mathnet-numerics/tree/master/src/NativeProviders">mathnet-numerics</a>. 
                        SharpNEAT could potentially use this to gain access to the abstractions that mathnet provides, thus supporting a widespread
                        range of options rather than tying SharpNEAT to one or two. However, a recent check discovered that the
                        Intel MKL provider did not have support for MKL's sparse matrix sub-routines, so this may have to be addressed
                        (not sure about the other providers, e.g CUDA in particular warrants strong attention).
                    </li>
                </ul>
                <br/>
                <hr/>
                <h4>Speciation Research</h4>
                <p>
                    Speciation by comparing genomes is possibly flawed in some significant ways; this fits the narrative around novelty search research,
                    and how following an objective function may not lead you to the desired objective.
                </p>
                <p>
                    For now the ideas under this heading are best covered by a number of fairly rambling blog posts, which I hope to condense
                    into something more concrete at some future time...
                </p>

                <ul>
                    <li><a href="http://redcalx.livejournal.com/140308.html">EC Notes</a></li>
                    <li><a href="http://redcalx.livejournal.com/140704.html">EC Notes II</a></li>
                    <li><a href="http://redcalx.livejournal.com/180809.html">Evolutionary Selection and N-Dimensional Density Fields</a></li>
                    <li><a href="http://redcalx.livejournal.com/181086.html">Evolutionary Strategy Space - General Approach for Evolved Neural Nets</a></li>
                    <li><a href="http://redcalx.livejournal.com/181438.html">Evolutionary Strategy Space - Sampling (Notes)</a></li>
                    <li><a href="http://redcalx.livejournal.com/181580.html">Evolutionary Strategy Space - Sampling II</a></li>
                </ul>
                <br/>
                <hr/>
            </div>
            <br />

            <!-- Notes / Miscellany  -->
            <div class="textbubble">
                <h3 align="center">Notes / Miscellany</h2>

                <h4>Performance - Floating Point Precision</h4>

                <p>The relative merits of single versus double precision floats.</p>
                <p>
                    There is an open question regarding how much precision is required in neuro-evolution methods.
                    E.g. for gradient descent learning, additional numerical precision certainly has benefits;
                    allowing more accurate representation of small values, and reduced rounding/numerical error generally.
                    There is clearly a trade-off between improved precision, and the extra storage space and memory transfer 
                    overhead and such.

                    In NEAT a very similar question arises - do the performance benefits offered by lower precision values 
                    outweigh the detrimental effects of the lower numeric precision?
                </p>
                <p>
                    A single precision float is 32 bits (4 bytes); double precision is 64bits (8 bytes). Therefore there
                    is a potential speed improvement to be gained by using less precision, in terms of fitting more weights
                    into CPU caches, efficient use of memory bandwidth and the ability to apply SIMD instructions to 2x as
                    many weights in one operation (at time of writing 256 bit SIMD instructions are common in consumer CPUs).
                </p>

                <p>A broader question might be whether the precision could be reduced further, given the existence of 
                <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">half precision floats</a></p>

                <h5>Notes / Links</h5>
                <ul>
                    <li><a href="https://devblogs.nvidia.com/parallelforall/new-features-cuda-7-5/">New Features in CUDA 7.5: 16-bit Floating Point (FP16) Data</a></li>
                    <li><a href="https://software.intel.com/en-us/articles/performance-benefits-of-half-precision-floats">Performance Benefits of Half Precision Floats</a></li>
                    <li><a href="https://en.m.wikipedia.org/wiki/Half-precision_floating-point_format#IEEE_754_half-precision_binary_floating-point_format:_binary16">IEEE 754 half-precision binary floating-point format: binary16</a></li>
                    <li><a href="http://stanford.edu/~rezab/nips2014workshop/slides/jeff.pdf">Techniques and Systems for Training Large Neural Networks Quickly, Jeff Dean, Google</a> - "Neural nets are very tolerant of reduced precision:
                        8 bits or less for inference 12 to 14 bits for training", pg 41</li>
                </ul>
                <h5>Response from <a href="http://sifter.org/~simon/journal/">Brandyn Webb</a></h5>
                <p>
                    Fwiw, I seem to recall productively reducing at least some of the weights
                    in Inkwell (Apple's handwriting recognition engine) to 8 bit logarithmic
                    values (expanded on the fly via a small lookup table).  We'd periodically
                    discretize them this way during training so that over time the weights could
                    compensate for each others' discretization errors (otherwise if you just lop
                    blindly at the end the odds are too high of systematic shifts that add up
                    over large numbers of weights).  In general I think the more abstract the
                    representation, the less granularity you need.  That is, whether a pixel is
                    brighter than its neighbor or not can require quite a precise measurement,
                    and this can matter, as can precise averages over a large number of noisy
                    pixels; but usually whether something is a dog or a tree, and the relative
                    implications thereof, is relatively high contrast--it's not that borderline
                    cases never exist, but just that they're increasingly rare as you ascend
                    the hierarchy of abstraction.  You also generally have far fewer examples
                    of more abstract concepts, so there's often little basis for a high precision
                    tally thereof.
                </p>
                <br/>
                <hr/>
            
                <h4>CPU Core Counts</h4>

                <p>
                    SharpNEAT is able to parallelize evaluation of genomes across multiple CPU cores. However, the efficiency of this approach likely falls
                    with increasing core counts. This is because the evolutionary algorithm operates on the basis of distinct generations, and therefore
                    there will typically be CPU cores with no work to do between generations if the work has not been evenly distributed between cores. To some
                    extent this depends on the evaluation scheme, i.e., if a single evaluation is long running and highly variable in duration, then it is much
                    harder to evenly distribute work across the CPU cores.
                </p>

                <br/>
                <hr/>

                <h4>Review 2D Physics Engine</h4>
                <p>
                    The current version of SharpNEAT (4.x) does not have any tasks with a 2D rendered visualix=zation, but in SharpNEAT 2.x the 
                    <a href="https://code.google.com/archive/p/box2dx/">box2dx</a> library was used for some of the tasks. There is a detailed 
                    overview of the situation regarding use of this 2D physics engine and the rendering library being used with it, 
                    at <a href="https://github.com/colgreen/box2dx/blob/master/README.md">gihub.com/colgreen/box2dx</a>.
                </p>
                <p> 
                    Ultimately it may be wise to switch to another 2D engine such as the <a href="https://github.com/codingben/box2d-netstandard">Box2D .NET Standard</a>.
                    For 3D physics and video rendering see <a href="https://github.com/bepu/bepuphysics2">bepuphysics v2</a>.
                </p>
                <br/>
                <hr/>

                <h4>Miscellany</h4>
                <ul>
                    <li>Replace roulette wheel selection with stochastic universal sampling (SUS).</li>
                    <li>Species visualisation(s).</li>
                    <li>Simple primer tutorial(s).</li>
                    <li>Periodic innovation ID defragmentation.</li>
                    <li>Distributed NEAT. Island model. (+ fast binary serialization/IO).</li>
                    <li>3D HyperNeat substrate/network visualization. Where # of connections is large visualization is possible by randomly thinning
                     out the shown connections. This approach is used in TrackVis (http://www.trackvis.org/) to visualise brain fibers.</li>
                </ul>
            </div>
        <br />
        <hr />
        <div class="frontpagefooter">
            Contact: <a href="mailto:colin.green1@gmail.com">Colin Green</a><br /></li>
            SharpNEAT is a <a href="http://heliosphan.org/">Heliosphan.org</a> project.<br /></li>
        </div>
        <br />
    </div>
</body>
</html>
